{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Word learn.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"qMHvOq64K6Vf","colab_type":"code","outputId":"9793af8c-7758-44ff-80eb-9f95769c26f9","executionInfo":{"status":"ok","timestamp":1558712428347,"user_tz":-120,"elapsed":1455,"user":{"displayName":"Notheryne","photoUrl":"","userId":"07099107331681534432"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":250,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ESQslvfOLUOl","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import re\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eX1awcFnMcJH","colab_type":"code","colab":{}},"source":["articles = pd.read_csv(\"/content/gdrive/My Drive/Word learn/all_data.csv\")\n","text = articles['Title'].str.cat(sep='\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQbRv7aKM02y","colab_type":"code","colab":{}},"source":["text = re.sub(\"\\\"\", \"\", text)\n","text = text.lower()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sWOneIrNN7qk","colab_type":"code","colab":{}},"source":["with open('/content/gdrive/My Drive/Word learn/input.txt', 'w', encoding = \"utf-8\") as wfile:\n","  wfile.write(text)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uc9EVrp4RqOp","colab_type":"code","colab":{}},"source":["from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, LSTM, Dense, Dropout\n","from keras.preprocessing.text import Tokenizer\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.models import Sequential, load_model\n","from keras.utils import to_categorical\n","import numpy as np\n","import math\n","from pickle import dump"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o_gwg6bQg0iq","colab_type":"code","colab":{}},"source":["#with open('/content/gdrive/My Drive/Word learn/input.txt', 'r', encoding = 'utf-8') as rfile:\n","#  data = rfile.read()\n","with open('/content/gdrive/My Drive/Word learn/test.txt', 'r', encoding = 'utf-8') as rfile:\n","  data = rfile.read()\n","\n","data = data.lower().split('\\n')\n","\n","lens = [len(x) for x in data]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FnHdKQatmMYk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"b57d26b4-3663-4def-f662-12697961ff4e","executionInfo":{"status":"ok","timestamp":1558717407769,"user_tz":-120,"elapsed":1656,"user":{"displayName":"Notheryne","photoUrl":"","userId":"07099107331681534432"}}},"source":["max_seq_len = max(lens)\n","batchsize = 32\n","total_words = sum(lens)\n","seq_num = len(data)\n","print(math.ceil(seq_num / batchsize))\n","data_batches = []\n","\n","for i in range(0, len(data), batchsize):\n","  data_batches.append(data[i:i+batchsize])\n","\n","batchnum = len(data_batches)\n","last_batch_size = len(data_batches[-1])\n","print(\"Number of sequences:\", seq_num)\n","print(\"The longest article (number of words):\", max_seq_len)\n","print(\"The size of a single batch (number of sequences):\", batchsize)\n","print(\"The amount of batches:\", batchnum)\n","print(\"The size of a last batch (number of sequences):\", last_batch_size)\n","print(\"The total amount of words:\", total_words)"],"execution_count":296,"outputs":[{"output_type":"stream","text":["6\n","Number of sequences: 172\n","The longest article (number of words): 29\n","The size of a single batch (number of sequences): 32\n","The amount of batches: 6\n","The size of a last batch (number of sequences): 12\n","The total amount of words: 4237\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mNr91AoOCxe8","colab_type":"code","colab":{}},"source":["tokenizer = Tokenizer()\n","def myGenerator(data_batches):\n","  X = []\n","  Y = []\n","  for batch in data_batches[:-1]:\n","    tokenizer.fit_on_texts(batch)\n","    total_batch_words = len(tokenizer.word_index) + 1\n","\n","    input_sequences = []\n","\n","    for line in batch:\n","      token_list = tokenizer.texts_to_sequences([line])[0]\n","      for i in range(1, len(token_list)):\n","        n_gram_sequences = token_list[:i+1]\n","        input_sequences.append(n_gram_sequences)\n","\n","    #pad\n","    input_sequences = np.array(pad_sequences(input_sequences, maxlen = max_seq_len, padding = 'pre'))\n","\n","    #predictors and label\n","    predictors, label = input_sequences[:,:-1], input_sequences[:,-1]\n","\n","    predictors = np.array(predictors)\n","\n","    #predictors = np.reshape(predictors, (predictors.shape[0], 1, predictors.shape[1]))\n","    label = to_categorical(label, num_classes = total_batch_words)\n","    #print(predictors, label)\n","    #print(predictors.shape, label.shape)\n","    #print(\"X shape: \", predictors.shape)\n","    #print(\"Y shape: \", label.shape)\n","    X.append(predictors)\n","    Y.append(label)\n","  return (X, Y)\n","      "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7zrJwg94rso5","colab_type":"code","colab":{}},"source":["def gen_myGenerator(data_batches):\n","  while True:\n","    for batch in data_batches[:-1]:\n","      tokenizer.fit_on_texts(batch)\n","      total_batch_words = len(tokenizer.word_index) + 1\n","\n","      input_sequences = []\n","\n","      for line in batch:\n","        token_list = tokenizer.texts_to_sequences([line])[0]\n","        for i in range(1, len(token_list)):\n","          n_gram_sequences = token_list[:i+1]\n","          input_sequences.append(n_gram_sequences)\n","\n","      #pad\n","      input_sequences = np.array(pad_sequences(input_sequences, maxlen = max_seq_len, padding = 'pre'))\n","\n","      #predictors and label\n","      predictors, label = input_sequences[:,:-1], input_sequences[:,-1]\n","\n","      predictors = np.array(predictors)\n","\n","      predictors = np.reshape(predictors, (predictors.shape[0], 1, predictors.shape[1]))\n","      label = to_categorical(label, num_classes = total_batch_words)\n","      #print(predictors, label)\n","      #print(predictors.shape, label.shape)\n","      #print(\"X shape: \", predictors.shape)\n","      #print(\"Y shape: \", label.shape)\n","      yield (predictors, label)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KfBJ7hRUTbNX","colab_type":"code","colab":{}},"source":["def create_model(my_generator, max_sequence_len, total_words, verb = 1):\n","  input_len = max_sequence_len - 1\n","\n","  model = Sequential()\n","  model.add(Embedding(28, 10, input_length=max_sequence_len-1))\n","  model.add(LSTM(300, return_sequences = True))\n","  #model.add(Dropout(0.2))\n","  model.add(LSTM(100))\n","  model.add(Dense(28, activation='softmax'))\n","\n","  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n","\n","  checkpoint_path = r\"/content/gdrive/My Drive/Word learn/Checkpoints/checkpt--{epoch:02d}.hdf5\"\n","  checkpoint = ModelCheckpoint(checkpoint_path, monitor='acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=5)\n","  earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n","  callbacks_list = [checkpoint, earlystop]\n","\n","  model.fit_generator(my_generator, steps_per_epoch = batchnum, epochs=100, callbacks = callbacks_list, verbose=verb)\n","\n","  full_model_path = r\"/content/gdrive/My Drive/Word learn/2layers_model.h5\"\n","  print(model.summary())\n","  model.save(full_model_path)\n","\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7zsD-Wd9dnVs","colab_type":"code","colab":{}},"source":["model = Sequential()\n","model.add(Embedding(total_words, 512, input_length = max_seq_len - 1))\n","#model.add(LSTM(34))#, return_sequences = True))\n","\n","model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xbDaPE0jYWIR","colab_type":"code","colab":{}},"source":["all_vec = myGenerator(data_batches)\n","\n","all_vec_reshaped = [(np.reshape(x[0], (x[0].shape[0], 1, x[0].shape[1])), np.reshape(x[1], (x[1].shape[0], 1, x[1].shape[1]))) for x in all_vec]\n","\n","for vec in all_vec:\n","  model.train_on_batch(vec[0], vec[1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lZPrTJBdyM_x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"outputId":"0204a189-5b5b-494d-d8df-b5ce07a5a3c7","executionInfo":{"status":"error","timestamp":1558717475099,"user_tz":-120,"elapsed":1780,"user":{"displayName":"Notheryne","photoUrl":"","userId":"07099107331681534432"}}},"source":["model.fit_generator(gen_myGenerator(data_batches), steps_per_epoch = batchnum, epochs = 10)"],"execution_count":298,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-298-e7e6c889bbf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_myGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_50_input to have 2 dimensions, but got array with shape (129, 1, 28)"]}]}]}